{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "35952a83-b46d-497a-8a2a-b3df0647abb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5b4ba708-ec8d-4673-a579-308827ebda90",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defines the biological network, Markov Decision Process and helper functions \n",
    "\n",
    "# We have 4 genes: [ATM, p53, Wip1, MDM2].\n",
    "# Each gene can be 0 (OFF) or 1 (ON).\n",
    "# 16 possible states, each is a 4D bit vector.\n",
    "\n",
    "def build_states():\n",
    "    \"\"\"\n",
    "    Returns a list of 16 states, each a 4-bit vector [b0,b1,b2,b3].\n",
    "    Also returns two dicts:\n",
    "      state_to_index[tuple_bits] = index in [0..15]\n",
    "      index_to_state[index] = tuple_bits\n",
    "    \"\"\"\n",
    "    states = []\n",
    "    for i in range(16):\n",
    "        bits = [(i >> b) & 1 for b in reversed(range(4))]  # [ATM,p53,Wip1,MDM2]\n",
    "        states.append(bits)\n",
    "    state_to_index = {}\n",
    "    index_to_state = {}\n",
    "    for idx, s_bits in enumerate(states):\n",
    "        t = tuple(s_bits)\n",
    "        state_to_index[t] = idx\n",
    "        index_to_state[idx] = t\n",
    "    return states, state_to_index, index_to_state\n",
    "\n",
    "# The action space A = {a1= [0,0,0,0], a2= [1,0,0,0], a3= [0,1,0,0], a4= [0,0,1,0], a5= [0,0,0,1]}\n",
    "    \n",
    "def build_actions():\n",
    "    \"\"\"\n",
    "    Returns a list of 5 actions (4D bit vectors), plus two dicts for index <-> action.\n",
    "    We'll map action indices to the labels a1, a2, a3, a4, a5 for printing.\n",
    "    \"\"\"\n",
    "    actions = [\n",
    "        [0,0,0,0],  # a1\n",
    "        [1,0,0,0],  # a2\n",
    "        [0,1,0,0],  # a3\n",
    "        [0,0,1,0],  # a4\n",
    "        [0,0,0,1],  # a5\n",
    "    ]\n",
    "    action_to_index = {}\n",
    "    # Create label dictionary: 0->\"a1\", 1->\"a2\", etc.\n",
    "    index_to_action = {0: \"a1\", 1: \"a2\", 2: \"a3\", 3: \"a4\", 4: \"a5\"}\n",
    "\n",
    "    for i, a_bits in enumerate(actions):\n",
    "        t = tuple(a_bits)\n",
    "        action_to_index[t] = i\n",
    "\n",
    "    return actions, action_to_index, index_to_action\n",
    "\n",
    "# Connectivity matrix C (from the figure)\n",
    "\n",
    "C = np.array([\n",
    "    [ 0,  0, -1,  0],\n",
    "    [ 1,  0, -1, -1],\n",
    "    [ 0,  1,  0,  0],\n",
    "    [-1,  1,  1,  0]\n",
    "], dtype=int)\n",
    "\n",
    "def threshold_and_convert(vec):\n",
    "    \"\"\"\n",
    "    For a 4D integer vector 'vec', apply:\n",
    "      if > 0 => 1, else => 0\n",
    "    Returns a 4D bit vector in {0,1}^4.\n",
    "    \"\"\"\n",
    "    out = []\n",
    "    for x in vec:\n",
    "        if x > 0:\n",
    "            out.append(1)\n",
    "        else:\n",
    "            out.append(0)\n",
    "    return out\n",
    "\n",
    "def xor_bits(vecA, vecB):\n",
    "    \"\"\"\n",
    "    Component-wise XOR for two 4D vectors in {0,1}.\n",
    "    Returns a new list of length 4.\n",
    "    \"\"\"\n",
    "    return [(a ^ b) for (a,b) in zip(vecA, vecB)]\n",
    "\n",
    "def hamming_distance(vecA, vecB):\n",
    "    \"\"\"\n",
    "    Return the number of components where vecA differs from vecB.\n",
    "    Both are length-4 bit vectors.\n",
    "    \"\"\"\n",
    "    return sum(a != b for a,b in zip(vecA, vecB))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cf5de6c2-ed03-49b7-8d71-9aa5fde9cb8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to define and build the transition model and reward model \n",
    "\n",
    "def build_transition_matrices(states, state_to_index, actions, p):\n",
    "    \"\"\"\n",
    "    Build a list of 5 transition matrices M(a), each shape (16,16).\n",
    "    M(a)[i,j] = Probability of going from state i to j if we pick action a.\n",
    "    \n",
    "    The formula from the problem is:\n",
    "       M(a)_{i,j} = p^d (1-p)^(4-d)\n",
    "    where d = HammingDistance( o_j ,  C*o_i (thresholded) XOR a ).\n",
    "    \"\"\"\n",
    "    num_states = 16\n",
    "    num_actions = 5\n",
    "    M = np.zeros((num_actions, num_states, num_states), dtype=float)\n",
    "\n",
    "    for a_idx, a_bits in enumerate(actions):\n",
    "        for i in range(num_states):\n",
    "            o_i = states[i]  # e.g. [0,1,0,1]\n",
    "            # 1) Compute C*o_i (as an integer vector), then threshold\n",
    "            ci = C.dot(o_i)  # shape (4,)\n",
    "            ci_thresh = threshold_and_convert(ci)  # in {0,1}^4\n",
    "            # 2) XOR with a\n",
    "            x = xor_bits(ci_thresh, a_bits)\n",
    "            \n",
    "            # For each possible next state j:\n",
    "            for j in range(num_states):\n",
    "                o_j = states[j]\n",
    "                d = hamming_distance(o_j, x)\n",
    "                prob = (p**d)*((1.0 - p)**(4-d))\n",
    "                M[a_idx, i, j] = prob\n",
    "    return M\n",
    "\n",
    "def build_reward_array(states, actions):\n",
    "    \"\"\"\n",
    "    R[s,a,s'] = 5*(#ON in s') - |a|.\n",
    "    where |a| is the sum of bits in action a.\n",
    "    \"\"\"\n",
    "    num_states = 16\n",
    "    num_actions = 5\n",
    "    R = np.zeros((num_actions, num_states, num_states), dtype=float)\n",
    "    for a_idx, a_bits in enumerate(actions):\n",
    "        cost_a = sum(a_bits)  # e.g. 0 or 1\n",
    "        for i in range(num_states):\n",
    "            for j in range(num_states):\n",
    "                o_j = states[j]\n",
    "                num_on = sum(o_j)\n",
    "                R[a_idx, i, j] = 5*num_on - cost_a\n",
    "    return R"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1486bf1e-f8a0-4a6d-82d5-d79184a53df9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Value Iteration Technique\n",
    "\n",
    "def value_iteration(M, R, gamma=0.95, theta=0.01):\n",
    "    \"\"\"\n",
    "    M: shape (5,16,16) => M[a,i,j]\n",
    "    R: shape (5,16,16) => R[a,i,j]\n",
    "    gamma: discount factor\n",
    "    theta: convergence threshold\n",
    "\n",
    "    Returns: (V, pi, num_iterations)\n",
    "      V: shape (16,) final value function\n",
    "      pi: shape (16,) optimal action index for each state\n",
    "      num_iterations: how many iterations until convergence\n",
    "    \"\"\"\n",
    "    num_actions, num_states, _ = M.shape\n",
    "    V = np.zeros(num_states)\n",
    "    iteration = 0\n",
    "\n",
    "    while True:\n",
    "        iteration += 1\n",
    "        V_new = np.zeros(num_states)\n",
    "        for i in range(num_states):\n",
    "            best_val = -1e9\n",
    "            # Check each action\n",
    "            for a_idx in range(num_actions):\n",
    "                Qa = 0.0\n",
    "                for j in range(num_states):\n",
    "                    prob = M[a_idx, i, j]\n",
    "                    r = R[a_idx, i, j]\n",
    "                    Qa += prob * (r + gamma * V[j])\n",
    "                if Qa > best_val:\n",
    "                    best_val = Qa\n",
    "            V_new[i] = best_val\n",
    "        delta = np.max(np.abs(V_new - V))\n",
    "        V = V_new\n",
    "        if delta < theta:\n",
    "            break\n",
    "\n",
    "    # Extract policy\n",
    "    pi = np.zeros(num_states, dtype=int)\n",
    "    for i in range(num_states):\n",
    "        best_a = 0\n",
    "        best_val = -1e9\n",
    "        for a_idx in range(num_actions):\n",
    "            Qa = 0.0\n",
    "            for j in range(num_states):\n",
    "                prob = M[a_idx, i, j]\n",
    "                r = R[a_idx, i, j]\n",
    "                Qa += prob * (r + gamma * V[j])\n",
    "            if Qa > best_val:\n",
    "                best_val = Qa\n",
    "                best_a = a_idx\n",
    "        pi[i] = best_a\n",
    "\n",
    "    return V, pi, iteration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "90565c5e-ddab-4c08-9cc3-aaacceffc24d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Policy Iteration Technique\n",
    "\n",
    "def policy_evaluation(pi, M, R, gamma=0.95, theta=0.01):\n",
    "    \"\"\"\n",
    "    Evaluate a fixed policy pi, shape (16,).\n",
    "    M[a,i,j], R[a,i,j].\n",
    "    Returns: V (shape (16,))\n",
    "    \"\"\"\n",
    "    num_states = M.shape[1]\n",
    "    V = np.zeros(num_states)\n",
    "    while True:\n",
    "        V_new = np.zeros(num_states)\n",
    "        for i in range(num_states):\n",
    "            a_idx = pi[i]\n",
    "            val = 0.0\n",
    "            for j in range(num_states):\n",
    "                prob = M[a_idx, i, j]\n",
    "                r = R[a_idx, i, j]\n",
    "                val += prob * (r + gamma*V[j])\n",
    "            V_new[i] = val\n",
    "        delta = np.max(np.abs(V_new - V))\n",
    "        V = V_new\n",
    "        if delta < theta:\n",
    "            break\n",
    "    return V\n",
    "\n",
    "def policy_improvement(V, M, R, gamma=0.95):\n",
    "    \"\"\"\n",
    "    Given V, compute pi_new by greedy improvement.\n",
    "    \"\"\"\n",
    "    num_actions, num_states, _ = M.shape\n",
    "    pi_new = np.zeros(num_states, dtype=int)\n",
    "    for i in range(num_states):\n",
    "        best_a = 0\n",
    "        best_val = -1e9\n",
    "        for a_idx in range(num_actions):\n",
    "            Qa = 0.0\n",
    "            for j in range(num_states):\n",
    "                prob = M[a_idx, i, j]\n",
    "                r = R[a_idx, i, j]\n",
    "                Qa += prob*(r + gamma*V[j])\n",
    "            if Qa > best_val:\n",
    "                best_val = Qa\n",
    "                best_a = a_idx\n",
    "        pi_new[i] = best_a\n",
    "    return pi_new\n",
    "\n",
    "def policy_iteration(M, R, gamma=0.95, theta=0.01):\n",
    "    \"\"\"\n",
    "    Returns: (V, pi, iteration_count)\n",
    "    \"\"\"\n",
    "    num_states = M.shape[1]\n",
    "    pi = np.zeros(num_states, dtype=int)  # start with a1 in all states\n",
    "    iteration = 0\n",
    "    while True:\n",
    "        iteration += 1\n",
    "        V = policy_evaluation(pi, M, R, gamma, theta)\n",
    "        pi_new = policy_improvement(V, M, R, gamma)\n",
    "        if np.array_equal(pi_new, pi):\n",
    "            break\n",
    "        pi = pi_new\n",
    "    return V, pi, iteration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b1f0bfb6-f769-48d5-963e-599850536e8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Functions to simulate the policy to measure average activation of the genes in the biological network\n",
    "\n",
    "def simulate_policy(pi, M, states, p, num_episodes=100, episode_length=200):\n",
    "    \"\"\"\n",
    "    Sample multiple episodes from the MDP using a fixed policy pi.\n",
    "    Return average number of ON genes (AvgA).\n",
    "    \"\"\"\n",
    "    num_states = 16\n",
    "    A_list = []\n",
    "    for _ in range(num_episodes):\n",
    "        s = np.random.randint(0, num_states)\n",
    "        total_on = 0\n",
    "        for step in range(episode_length):\n",
    "            o_bits = states[s]\n",
    "            total_on += sum(o_bits)\n",
    "            a_idx = pi[s]\n",
    "            probs = M[a_idx, s, :]\n",
    "            s_next = np.random.choice(num_states, p=probs)\n",
    "            s = s_next\n",
    "        A_i = total_on / float(episode_length)\n",
    "        A_list.append(A_i)\n",
    "    return np.mean(A_list)\n",
    "\n",
    "def simulate_no_control(M, states, p, num_episodes=100, episode_length=200):\n",
    "    \"\"\"\n",
    "    \"No control\" => always action a1 => index=0.\n",
    "    So the transition matrix is M[0].\n",
    "    \"\"\"\n",
    "    num_states = 16\n",
    "    A_list = []\n",
    "    for _ in range(num_episodes):\n",
    "        s = np.random.randint(0, num_states)\n",
    "        total_on = 0\n",
    "        for step in range(episode_length):\n",
    "            o_bits = states[s]\n",
    "            total_on += sum(o_bits)\n",
    "            probs = M[0, s, :]\n",
    "            s_next = np.random.choice(num_states, p=probs)\n",
    "            s = s_next\n",
    "        A_i = total_on / float(episode_length)\n",
    "        A_list.append(A_i)\n",
    "    return np.mean(A_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2c634914-e2da-42c2-b5f2-aed0ccd7dab8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Value Iteration for p=0.04:\n",
      " - Converged in 142 iterations.\n",
      " - Final Value Function (16 states) = [258.4 258.4 262.8 262.8 267.7 267.7 267.7 267.7 263.1 258.4 258.4 258.4\n",
      " 267.7 263.1 267.7 267.7]\n",
      " - Final Policy (16 states) = ['a3', 'a3', 'a3', 'a3', 'a3', 'a3', 'a3', 'a3', 'a4', 'a3', 'a3', 'a3', 'a5', 'a3', 'a3', 'a3']\n",
      " - Avg Activation no control = 0.42\n",
      " - Avg Activation optimal    = 2.87\n",
      "\n",
      "Value Iteration for p=0.15:\n",
      " - Converged in 139 iterations.\n",
      " - Final Value Function (16 states) = [227.6 227.6 230.8 230.8 234.8 234.8 234.8 234.8 231.4 227.6 227.6 227.6\n",
      " 234.8 231.4 234.8 234.8]\n",
      " - Final Policy (16 states) = ['a3', 'a3', 'a3', 'a3', 'a3', 'a3', 'a3', 'a3', 'a4', 'a3', 'a3', 'a3', 'a5', 'a3', 'a3', 'a3']\n",
      " - Avg Activation no control = 1.04\n",
      " - Avg Activation optimal    = 2.54\n",
      "\n",
      "Value Iteration for p=0.48:\n",
      " - Converged in 136 iterations.\n",
      " - Final Value Function (16 states) = [196.  196.  196.2 196.2 196.4 196.4 196.4 196.4 196.2 196.  196.  196.\n",
      " 196.4 196.2 196.4 196.4]\n",
      " - Final Policy (16 states) = ['a1', 'a1', 'a1', 'a1', 'a1', 'a1', 'a1', 'a1', 'a1', 'a1', 'a1', 'a1', 'a1', 'a1', 'a1', 'a1']\n",
      " - Avg Activation no control = 1.97\n",
      " - Avg Activation optimal    = 1.97\n",
      "\n",
      "Policy Iteration for p=0.05:\n",
      " - Converged in 3 policy improvements.\n",
      " - Final Value Function (16 states) = [255.5 255.5 259.8 259.8 264.6 264.6 264.6 264.6 260.1 255.5 255.5 255.5\n",
      " 264.6 260.1 264.6 264.6]\n",
      " - Final Policy (16 states) = ['a3', 'a3', 'a3', 'a3', 'a3', 'a3', 'a3', 'a3', 'a4', 'a3', 'a3', 'a3', 'a5', 'a3', 'a3', 'a3']\n",
      " - Avg Activation no control = 0.48\n",
      " - Avg Activation optimal    = 2.84\n",
      "\n",
      "(Compare with Value Iteration at p=0.05):\n",
      "\n",
      "Value Iteration for p=0.05:\n",
      " - Converged in 142 iterations.\n",
      " - Final Value Function (16 states) = [255.5 255.5 259.8 259.8 264.6 264.6 264.6 264.6 260.1 255.5 255.5 255.5\n",
      " 264.6 260.1 264.6 264.6]\n",
      " - Final Policy (16 states) = ['a3', 'a3', 'a3', 'a3', 'a3', 'a3', 'a3', 'a3', 'a4', 'a3', 'a3', 'a3', 'a5', 'a3', 'a3', 'a3']\n",
      " - Avg Activation no control = 0.48\n",
      " - Avg Activation optimal    = 2.84\n"
     ]
    }
   ],
   "source": [
    "# Implement value iteration and policy iteration for given scenarios\n",
    "\n",
    "def main():\n",
    "    # Build states & actions\n",
    "    states, state_to_index, index_to_state = build_states()\n",
    "    actions, action_to_index, index_to_action = build_actions()\n",
    "\n",
    "    # Define a helper to run Value Iteration for a given p, then test the resulting policy.\n",
    "    def run_value_iteration_for_p(p):\n",
    "        # Build M(a), R(a)\n",
    "        M = build_transition_matrices(states, state_to_index, actions, p)\n",
    "        R = build_reward_array(states, actions)\n",
    "        \n",
    "        # Run value iteration\n",
    "        V, pi, iters = value_iteration(M, R, gamma=0.95, theta=0.01)\n",
    "        \n",
    "        # Convert numeric pi to labeled actions (e.g. \"a1\", \"a2\", etc.)\n",
    "        labeled_pi = [f\"a{a_idx+1}\" for a_idx in pi]\n",
    "\n",
    "        # Compare with no control\n",
    "        avgA_noctrl = simulate_no_control(M, states, p)\n",
    "        \n",
    "        # Compare with optimal policy\n",
    "        avgA_opt = simulate_policy(pi, M, states, p)\n",
    "\n",
    "        print(f\"\\nValue Iteration for p={p}:\")\n",
    "        print(f\" - Converged in {iters} iterations.\")\n",
    "        print(\" - Final Value Function (16 states) =\", np.round(V,1))\n",
    "        print(\" - Final Policy (16 states) =\", labeled_pi)\n",
    "        print(f\" - Avg Activation no control = {avgA_noctrl:.2f}\")\n",
    "        print(f\" - Avg Activation optimal    = {avgA_opt:.2f}\")\n",
    "\n",
    "    # Part (a) => p=0.04\n",
    "    run_value_iteration_for_p(0.04)\n",
    "\n",
    "    # Part (b) => p=0.15, p=0.48\n",
    "    run_value_iteration_for_p(0.15)\n",
    "    run_value_iteration_for_p(0.48)\n",
    "\n",
    "    # Part (c) => p=0.05, do Policy Iteration (starting with a1 in all states)\n",
    "    \n",
    "    p_c = 0.05\n",
    "    M_c = build_transition_matrices(states, state_to_index, actions, p_c)\n",
    "    R_c = build_reward_array(states, actions)\n",
    "    V_pi, pi_pi, pi_iters = policy_iteration(M_c, R_c, gamma=0.95, theta=0.01)\n",
    "    \n",
    "    # Also compare no control vs. policy iteration result\n",
    "    labeled_pi_pi = [f\"a{a_idx+1}\" for a_idx in pi_pi]\n",
    "    avgA_noctrl_c = simulate_no_control(M_c, states, p_c)\n",
    "    avgA_opt_c = simulate_policy(pi_pi, M_c, states, p_c)\n",
    "\n",
    "    print(f\"\\nPolicy Iteration for p={p_c}:\")\n",
    "    print(f\" - Converged in {pi_iters} policy improvements.\")\n",
    "    print(\" - Final Value Function (16 states) =\", np.round(V_pi,1))\n",
    "    print(\" - Final Policy (16 states) =\", labeled_pi_pi)\n",
    "    print(f\" - Avg Activation no control = {avgA_noctrl_c:.2f}\")\n",
    "    print(f\" - Avg Activation optimal    = {avgA_opt_c:.2f}\")\n",
    "\n",
    "    # Compare with the Value Iteration result for p=0.05,\n",
    "    print(\"\\n(Compare with Value Iteration at p=0.05):\")\n",
    "    run_value_iteration_for_p(0.05)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f8037d0-3c6d-46c1-8def-def4e82049d9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
